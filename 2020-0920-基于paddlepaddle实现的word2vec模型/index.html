<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Mr.Su">
    
    <title>
        
            使用paddlepaddle实现word2vec模型 |
        
        Sunan&#39;s blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/null">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/fontawesome.min.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/regular.min.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/solid.min.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/brands.min.css">
    <script id="hexo-configurations">
    let KEEP = window.KEEP || {}
    KEEP.hexo_config = {"hostname":"sunan.me","root":"/","language":"en"}
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true,"init_open":true},"style":{"primary_color":"#5390fe","logo":null,"favicon":null,"avatar":"/images/avatar.svg","font_size":"18px","font_family":"STSong, Times New Roman","hover":{"shadow":true,"scale":true},"first_screen":{"enable":false,"header_transparent":true,"background_img":"/images/bg.svg","description":null,"font_color":null,"hitokoto":false},"scroll":{"progress_bar":true,"percent":false}},"local_search":{"enable":false,"preload":false},"code_copy":{},"code_block":{"tools":{"enable":true,"style":"mac"},"highlight_theme":"default"},"side_tools":{},"pjax":{"enable":false},"lazyload":{"enable":true},"comment":{"enable":false,"use":"valine","valine":{"appid":null,"appkey":null,"server_urls":null,"placeholder":null},"gitalk":{"github_id":null,"github_admins":null,"repository":null,"client_id":null,"client_secret":null,"proxy":null},"twikoo":{"env_id":null,"region":null,"version":"1.6.8"},"waline":{"server_url":null,"reaction":false,"version":2}},"post":{"author_label":{"enable":false,"auto":false,"custom_label_list":["Trainee","Engineer","Architect"]},"word_count":{"enable":true,"wordcount":true,"min2read":true},"img_align":"left","copyright_info":true},"version":"3.6.1"}
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"}
    KEEP.language_code_block = {"copy":"Copy code","copied":"Copied","fold":"Fold code block","folded":"Folded"}
    KEEP.language_copy_copyright = {"copy":"Copy copyright info","copied":"Copied","title":"Original article title","author":"Original article author","link":"Original article link"}
  </script>
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            
<header class="header-wrapper">

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/">
               Sunan&#39;s blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                ARCHIVES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/about"
                            >
                                ABOUT
                            </a>
                        </li>
                    
                    
                </ul>
            </div>
            <div class="mobile">
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/about">ABOUT</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">

            <div class="article-title">
                <span class="title-hover-animation">使用paddlepaddle实现word2vec模型</span>
            </div>

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/avatar.svg">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">Mr.Su</span>
                            
                        </div>
                        <div class="meta-info">
                            
<div class="article-meta-info">
    <span class="article-date article-meta-item">
        
            <i class="fa-regular fa-calendar-plus"></i>&nbsp;
        
        <span class="pc">2020-09-20 10:55:00</span>
        <span class="mobile">2020-09-20 10:55</span>
    </span>
    
        <span class="article-update-date article-meta-item">
        <i class="fas fa-file-pen"></i>&nbsp;
        <span class="pc">2020-09-20 13:41:00</span>
    </span>
    
    
        <span class="article-categories article-meta-item">
            <i class="fas fa-folder"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fas fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/python/">python</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/NLP/">NLP</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content keep-markdown-body">
                

                <h3 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h3><h4 id="Embeddings"><a href="#Embeddings" class="headerlink" title="Embeddings"></a>Embeddings</h4><p> Embedding（嵌入）是拓扑学里面的词。</p>
<p>在深度学习中的意思是使用低纬度的的数据表示高纬度的数据。例如，在NLP中，“ I have an apple ”这句话中的每个单词可以分别用4维向量表示。</p>
<p>I [1,0,0,0]</p>
<p>have [0,1,0,0]</p>
<p>an [0,0,1,0]</p>
<p>apple [0,0,0,1]</p>
<p>这种方法叫做word embedding.</p>
<h4 id="One-Hot编码"><a href="#One-Hot编码" class="headerlink" title="One-Hot编码"></a>One-Hot编码</h4><p>one-hot编码是将文本转化成二级制的一种编码。在训练word2vec模型之前，我们需要利用训练数据构建自己的词汇表，在对词汇表进行one-hot编码。</p>
<span id="more"></span>

<p>例如： 假设从我们的训练文档中抽取出100个唯一不重复的单词组成词汇表。我们对这100个单词进行one-hot编码，得到的每个单词都是一个100维的向量，向量每个维度的值只有0或者1，假如单词have在词汇表中的出现位置为第3个，那么have的one-hot表示就是一个第三维度取值为1，其他维都为0的100维的向量。 </p>
<h3 id="word2vec原理简述"><a href="#word2vec原理简述" class="headerlink" title="word2vec原理简述"></a>word2vec原理简述</h3><h4 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h4><p>现在word2vec已经成为了自然语言处理领域的基础，在他之后的预训练模型或多或少都借鉴了他的一部分思想。</p>
<p>在word2vec训练完成之后，我们并不会使用这个模型处理新的任务，而是需要它的参数矩阵。word2vec模型有两种训练方法，skip-gram和CBOW。本文主要介绍使用skip-gram方法及其实现代码。</p>
<h4 id="skip-gram"><a href="#skip-gram" class="headerlink" title="skip-gram"></a>skip-gram</h4><p>如下图所示，直观理解skip-gram是给定input word来预测上下文。</p>
<p><img src="https://external-link.sunan.me/blog/200920/i8jLgj1bgi.png?imageslim" alt="mark"></p>
<h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><p>我们拿“I have an apple”这句话举例。</p>
<ol>
<li>首先选一个中间词作为input word，例如我们选择了have。</li>
<li>定义一个参数skip_windows ，它代表着从input word 的一侧（左或者右）选取词的数量，例如我们选取skip_windows &#x3D; 2,那么获得的窗口中的词(包换input word)是[I,have,an,apple]</li>
<li>定义一个参数num_skips ,他代表从整个窗口中选用多少次作为output word.例如当skip_windows &#x3D;2,并且num_skips &#x3D;2时，我们会得到两组{input word,output wold}。</li>
<li>神经网络通过这些数据输出一个概率分布，这个概率代表着我们词典中每个词是output wold的可能性。</li>
</ol>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>网络结构定义代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle.fluid <span class="keyword">as</span> fluid</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">skip_gram_word2vec</span>(<span class="params">dict_size, embedding_size, is_sparse=<span class="literal">False</span>, neg_num=<span class="number">5</span></span>):</span><br><span class="line">    datas = []</span><br><span class="line">    input_word = fluid.layers.data(name=<span class="string">&quot;input_word&quot;</span>, shape=[<span class="number">1</span>], dtype=<span class="string">&#x27;int64&#x27;</span>)</span><br><span class="line">    true_word = fluid.layers.data(name=<span class="string">&#x27;true_label&#x27;</span>, shape=[<span class="number">1</span>], dtype=<span class="string">&#x27;int64&#x27;</span>)</span><br><span class="line">    neg_word = fluid.layers.data(</span><br><span class="line">        name=<span class="string">&quot;neg_label&quot;</span>, shape=[neg_num], dtype=<span class="string">&#x27;int64&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    datas.append(input_word)</span><br><span class="line">    datas.append(true_word)</span><br><span class="line">    datas.append(neg_word)</span><br><span class="line"></span><br><span class="line">    py_reader = fluid.layers.create_py_reader_by_data(</span><br><span class="line">        capacity=<span class="number">64</span>, feed_list=datas, name=<span class="string">&#x27;py_reader&#x27;</span>, use_double_buffer=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    words = fluid.layers.read_file(py_reader)</span><br><span class="line">    init_width = <span class="number">0.5</span> / embedding_size</span><br><span class="line">    input_emb = fluid.layers.embedding(</span><br><span class="line">        <span class="built_in">input</span>=words[<span class="number">0</span>],</span><br><span class="line">        is_sparse=is_sparse,</span><br><span class="line">        size=[dict_size, embedding_size],</span><br><span class="line">        param_attr=fluid.ParamAttr(</span><br><span class="line">            name=<span class="string">&#x27;emb&#x27;</span>,</span><br><span class="line">            initializer=fluid.initializer.Uniform(-init_width, init_width)))</span><br><span class="line"></span><br><span class="line">    true_emb_w = fluid.layers.embedding(</span><br><span class="line">        <span class="built_in">input</span>=words[<span class="number">1</span>],</span><br><span class="line">        is_sparse=is_sparse,</span><br><span class="line">        size=[dict_size, embedding_size],</span><br><span class="line">        param_attr=fluid.ParamAttr(</span><br><span class="line">            name=<span class="string">&#x27;emb_w&#x27;</span>, initializer=fluid.initializer.Constant(value=<span class="number">0.0</span>)))</span><br><span class="line"></span><br><span class="line">    true_emb_b = fluid.layers.embedding(</span><br><span class="line">        <span class="built_in">input</span>=words[<span class="number">1</span>],</span><br><span class="line">        is_sparse=is_sparse,</span><br><span class="line">        size=[dict_size, <span class="number">1</span>],</span><br><span class="line">        param_attr=fluid.ParamAttr(</span><br><span class="line">            name=<span class="string">&#x27;emb_b&#x27;</span>, initializer=fluid.initializer.Constant(value=<span class="number">0.0</span>)))</span><br><span class="line">    neg_word_reshape = fluid.layers.reshape(words[<span class="number">2</span>], shape=[-<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">    neg_word_reshape.stop_gradient = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    neg_emb_w = fluid.layers.embedding(</span><br><span class="line">        <span class="built_in">input</span>=neg_word_reshape,</span><br><span class="line">        is_sparse=is_sparse,</span><br><span class="line">        size=[dict_size, embedding_size],</span><br><span class="line">        param_attr=fluid.ParamAttr(</span><br><span class="line">            name=<span class="string">&#x27;emb_w&#x27;</span>, learning_rate=<span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line">    neg_emb_w_re = fluid.layers.reshape(</span><br><span class="line">        neg_emb_w, shape=[-<span class="number">1</span>, neg_num, embedding_size])</span><br><span class="line">    neg_emb_b = fluid.layers.embedding(</span><br><span class="line">        <span class="built_in">input</span>=neg_word_reshape,</span><br><span class="line">        is_sparse=is_sparse,</span><br><span class="line">        size=[dict_size, <span class="number">1</span>],</span><br><span class="line">        param_attr=fluid.ParamAttr(</span><br><span class="line">            name=<span class="string">&#x27;emb_b&#x27;</span>, learning_rate=<span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line">    neg_emb_b_vec = fluid.layers.reshape(neg_emb_b, shape=[-<span class="number">1</span>, neg_num])</span><br><span class="line">    true_logits = fluid.layers.elementwise_add(</span><br><span class="line">        fluid.layers.reduce_sum(</span><br><span class="line">            fluid.layers.elementwise_mul(input_emb, true_emb_w),</span><br><span class="line">            dim=<span class="number">1</span>,</span><br><span class="line">            keep_dim=<span class="literal">True</span>),</span><br><span class="line">        true_emb_b)</span><br><span class="line">    input_emb_re = fluid.layers.reshape(</span><br><span class="line">        input_emb, shape=[-<span class="number">1</span>, <span class="number">1</span>, embedding_size])</span><br><span class="line">    neg_matmul = fluid.layers.matmul(</span><br><span class="line">        input_emb_re, neg_emb_w_re, transpose_y=<span class="literal">True</span>)</span><br><span class="line">    neg_matmul_re = fluid.layers.reshape(neg_matmul, shape=[-<span class="number">1</span>, neg_num])</span><br><span class="line">    neg_logits = fluid.layers.elementwise_add(neg_matmul_re, neg_emb_b_vec)</span><br><span class="line">    <span class="comment">#nce loss</span></span><br><span class="line"></span><br><span class="line">    label_ones = fluid.layers.fill_constant_batch_size_like(</span><br><span class="line">        true_logits, shape=[-<span class="number">1</span>, <span class="number">1</span>], value=<span class="number">1.0</span>, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    label_zeros = fluid.layers.fill_constant_batch_size_like(</span><br><span class="line">        true_logits, shape=[-<span class="number">1</span>, neg_num], value=<span class="number">0.0</span>, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    true_xent = fluid.layers.sigmoid_cross_entropy_with_logits(true_logits,</span><br><span class="line">                                                               label_ones)</span><br><span class="line">    neg_xent = fluid.layers.sigmoid_cross_entropy_with_logits(neg_logits,</span><br><span class="line">                                                              label_zeros)</span><br><span class="line">    cost = fluid.layers.elementwise_add(</span><br><span class="line">        fluid.layers.reduce_sum(</span><br><span class="line">            true_xent, dim=<span class="number">1</span>),</span><br><span class="line">        fluid.layers.reduce_sum(</span><br><span class="line">            neg_xent, dim=<span class="number">1</span>))</span><br><span class="line">    avg_cost = fluid.layers.reduce_mean(cost)</span><br><span class="line">    <span class="keyword">return</span> avg_cost, py_reader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">infer_network</span>(<span class="params">vocab_size, emb_size</span>):</span><br><span class="line">    analogy_a = fluid.layers.data(name=<span class="string">&quot;analogy_a&quot;</span>, shape=[<span class="number">1</span>], dtype=<span class="string">&#x27;int64&#x27;</span>)</span><br><span class="line">    analogy_b = fluid.layers.data(name=<span class="string">&quot;analogy_b&quot;</span>, shape=[<span class="number">1</span>], dtype=<span class="string">&#x27;int64&#x27;</span>)</span><br><span class="line">    analogy_c = fluid.layers.data(name=<span class="string">&quot;analogy_c&quot;</span>, shape=[<span class="number">1</span>], dtype=<span class="string">&#x27;int64&#x27;</span>)</span><br><span class="line">    all_label = fluid.layers.data(</span><br><span class="line">        name=<span class="string">&quot;all_label&quot;</span>,</span><br><span class="line">        shape=[vocab_size, <span class="number">1</span>],</span><br><span class="line">        dtype=<span class="string">&#x27;int64&#x27;</span>,</span><br><span class="line">        append_batch_size=<span class="literal">False</span>)</span><br><span class="line">    emb_all_label = fluid.layers.embedding(</span><br><span class="line">        <span class="built_in">input</span>=all_label, size=[vocab_size, emb_size], param_attr=<span class="string">&quot;emb&quot;</span>)</span><br><span class="line"></span><br><span class="line">    emb_a = fluid.layers.embedding(</span><br><span class="line">        <span class="built_in">input</span>=analogy_a, size=[vocab_size, emb_size], param_attr=<span class="string">&quot;emb&quot;</span>)</span><br><span class="line">    emb_b = fluid.layers.embedding(</span><br><span class="line">        <span class="built_in">input</span>=analogy_b, size=[vocab_size, emb_size], param_attr=<span class="string">&quot;emb&quot;</span>)</span><br><span class="line">    emb_c = fluid.layers.embedding(</span><br><span class="line">        <span class="built_in">input</span>=analogy_c, size=[vocab_size, emb_size], param_attr=<span class="string">&quot;emb&quot;</span>)</span><br><span class="line">    target = fluid.layers.elementwise_add(</span><br><span class="line">        fluid.layers.elementwise_sub(emb_b, emb_a), emb_c)</span><br><span class="line">    emb_all_label_l2 = fluid.layers.l2_normalize(x=emb_all_label, axis=<span class="number">1</span>)</span><br><span class="line">    dist = fluid.layers.matmul(x=target, y=emb_all_label_l2, transpose_y=<span class="literal">True</span>)</span><br><span class="line">    values, pred_idx = fluid.layers.topk(<span class="built_in">input</span>=dist, k=<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">return</span> values, pred_idx</span><br></pre></td></tr></table></figure>



<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>[1]  Rong X. word2vec parameter learning explained[J]. arXiv preprint arXiv:1411.2738, 2014. </p>
<p>[2]  <a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/37471802" >https://zhuanlan.zhihu.com/p/37471802<i class="fas fa-external-link-alt"></i></a> </p>
<p>[3]  <a class="link"   target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/projectdetail/978573" >https://aistudio.baidu.com/aistudio/projectdetail/978573<i class="fas fa-external-link-alt"></i></a> </p>

            </div>

            
                <div class="post-copyright-info">
                    
<div class="article-copyright-info-container">
    <ul class="copyright-info-content">
        <li class="post-title">
            <span class="type">Post title</span>: <span class="content">使用paddlepaddle实现word2vec模型</span>
        </li>
        <li class="post-author">
            <span class="type">Post author</span>: <span class="content">Mr.Su</span>
        </li>
        <li class="post-time">
            <span class="type">Create time</span>: <span class="content">2020-09-20 10:55:00</span>
        </li>
        <li class="post-link">
            <span class="type">Post link</span>: <span class="content">2020-0920-基于paddlepaddle实现的word2vec模型/</span>
        </li>
        <li class="post-license">
            <span class="type">Copyright notice</span>: <span class="content">All articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> unless stating additionally.</span>
        </li>
    </ul>
    <div class="copy-copyright-info flex-center tooltip" data-content="Copy copyright info" data-offset-y="-2px">
        <i class="fa-solid fa-copy"></i>
    </div>
</div>

                </div>
            

            
                <ul class="post-tags-box">
                    
                        <li class="tag-item">
                            <a href="/tags/python/">#python</a>&nbsp;
                        </li>
                    
                        <li class="tag-item">
                            <a href="/tags/NLP/">#NLP</a>&nbsp;
                        </li>
                    
                </ul>
            

            
                <div class="article-nav">
                    
                        <div class="article-prev">
                            <a class="prev"
                               rel="prev"
                               href="/2021-0328-%E5%88%A9%E7%94%A8WSL%E5%9C%A8windows%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%AE%89%E8%A3%85SpiningUp/"
                            >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                                <span class="title flex-center">
                                <span class="post-nav-title-item">利用WSL在window环境下安装SpiningUp.md</span>
                                <span class="post-nav-item">Prev posts</span>
                            </span>
                            </a>
                        </div>
                    
                    
                        <div class="article-next">
                            <a class="next"
                               rel="next"
                               href="/2020-0803-SpringBoot%E4%B8%AD%E9%9D%99%E6%80%81%E7%B1%BB%E8%B0%83%E7%94%A8%E8%87%AA%E5%8A%A8%E6%B3%A8%E5%85%A5%E7%9A%84Mapper/"
                            >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">SpringBoot中静态类调用自动注入的Mapper</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                                <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                            </a>
                        </div>
                    
                </div>
            

            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%83%8C%E6%99%AF%E7%9F%A5%E8%AF%86"><span class="nav-number">1.</span> <span class="nav-text">背景知识</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Embeddings"><span class="nav-number">1.1.</span> <span class="nav-text">Embeddings</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#One-Hot%E7%BC%96%E7%A0%81"><span class="nav-number">1.2.</span> <span class="nav-text">One-Hot编码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#word2vec%E5%8E%9F%E7%90%86%E7%AE%80%E8%BF%B0"><span class="nav-number">2.</span> <span class="nav-text">word2vec原理简述</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2"><span class="nav-number">2.1.</span> <span class="nav-text">写在前面</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#skip-gram"><span class="nav-number">2.2.</span> <span class="nav-text">skip-gram</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-number">2.3.</span> <span class="nav-text">训练过程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81"><span class="nav-number">3.</span> <span class="nav-text">代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">4.</span> <span class="nav-text">参考资料</span></a></li></ol>
    </div>
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            
<footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
                <span>2015</span> -
            
            2023
            
                &nbsp;<i class="fas fa-heart icon-animate"></i>
                &nbsp;<a href="/">Mr.Su</a>
            
        </div>
        
        <div class="theme-info info-item">
            Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep</a>
        </div>
        
            <div class="icp-info info-item">
                <a target="_blank" rel="nofollow"
                   href="https://beian.miit.gov.cn"
                >
                    鲁ICP备15022835号
                </a>
            </div>
        
        
            <div class="deploy-info info-item">
                
                    <a target="_blank" rel="nofollow" href="https://github.com/mofengboy">
                
                    This site is provided with deployment services by <span class="tooltip" data-content="GitHub Pages"><img src="/images/deploy-provider/github.png"></span>
                
                    </a>
                
            </div>
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item flex-center toggle-show-toc">
                <i class="fas fa-list"></i>
            </li>
        

        <!-- go comment -->
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    <div class="zoom-in-image-mask">
    <img class="zoom-in-image">
</div>


    

</main>



<script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/utils.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/main.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/header-shrink.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/back2top.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/dark-light-toggle.js"></script>






    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/code-block.js"></script>



    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/lazyload.js"></script>


<div class="post-scripts">
    
        <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/post-helper.js"></script>
        
            <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/libs/anime.min.js"></script>
        
        
            <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/toc.js"></script>
        
    
</div>



</body>
</html>
